{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8190d732",
   "metadata": {},
   "source": [
    "## 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526669a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"[OK] ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab93cc",
   "metadata": {},
   "source": [
    "## 2. ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆBBC Newsï¼‰ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c707e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBC News Dataset ã‚’sklearnã‹ã‚‰å–å¾—\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"[INFO] ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "\n",
    "# 20 Newsgroupsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå¤§è¦æ¨¡ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼‰\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.med',\n",
    "    'talk.politics.guns'\n",
    "]\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "# DataFrameã«å¤‰æ›\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups.data,\n",
    "    'category': [categories[i] for i in newsgroups.target],\n",
    "    'category_id': newsgroups.target\n",
    "})\n",
    "\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ï¼ˆæœ€åˆã®500æ–‡å­—ã«åˆ¶é™ï¼‰\n",
    "df['text'] = df['text'].str[:500]\n",
    "\n",
    "print(f\"[OK] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)} ä»¶\")\n",
    "print(f\"\\nã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"\\nã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ:\")\n",
    "print(df['text'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613a9f5",
   "metadata": {},
   "source": [
    "## 3. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e62aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor.text_cleaner import TextCleaner\n",
    "\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "df['cleaned_text'] = df['text'].apply(TextCleaner.clean)\n",
    "\n",
    "print(\"[OK] ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†\")\n",
    "print(f\"\\nå‡¦ç†å‰å¾Œã®æ¯”è¼ƒ:\")\n",
    "print(f\"å‡¦ç†å‰: {df['text'].iloc[0][:100]}...\")\n",
    "print(f\"å‡¦ç†å¾Œ: {df['cleaned_text'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c245f1",
   "metadata": {},
   "source": [
    "## 4. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor.tokenizer import Tokenizer\n",
    "\n",
    "# ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆç©ºç™½åˆ†å‰²ï¼‰\n",
    "# æ³¨: æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã§ã¯ãªã„ãŸã‚ã€è‹±èªã®ç©ºç™½åˆ†å‰²ã‚’ä½¿ç”¨\n",
    "df['tokens'] = df['cleaned_text'].apply(lambda x: x.lower().split())\n",
    "\n",
    "print(\"[OK] ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å®Œäº†\")\n",
    "print(f\"\\nã‚µãƒ³ãƒ—ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³:\")\n",
    "print(df['tokens'].iloc[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9d051",
   "metadata": {},
   "source": [
    "## 5. ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆSentence-Transformersï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68983909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.vectorizer import TextVectorizer\n",
    "\n",
    "print(\"[INFO] ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œä¸­...\")\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆæœ€åˆã®300ä»¶ã‚’ä½¿ç”¨ã—ã¦è¨ˆç®—æ™‚é–“ã‚’çŸ­ç¸®ï¼‰\n",
    "vectorizer = TextVectorizer(use_mock=True)\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«å‰Šæ¸›\n",
    "df_sample = df.head(300).copy()\n",
    "texts = df_sample['cleaned_text'].tolist()\n",
    "embeddings = vectorizer.encode(texts, normalize=True)\n",
    "\n",
    "print(f\"[OK] ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†\")\n",
    "print(f\"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embeddings.shape[1]}\")\n",
    "print(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {embeddings.shape[0]}\")\n",
    "print(f\"ã‚µãƒ³ãƒ—ãƒ«åŸ‹ã‚è¾¼ã¿ï¼ˆæœ€åˆã®10æ¬¡å…ƒï¼‰: {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2948184e",
   "metadata": {},
   "source": [
    "## 6. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆHDBSCANï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae70467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.clustering import TopicClusterer\n",
    "\n",
    "print(\"[INFO] HDBSCAN ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n",
    "\n",
    "clusterer = TopicClusterer(min_cluster_size=5, min_samples=2)\n",
    "clusterer.fit(embeddings)\n",
    "\n",
    "df_sample['cluster'] = clusterer.get_cluster_labels()\n",
    "\n",
    "print(f\"[OK] ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†\")\n",
    "print(f\"æ¤œå‡ºã‚¯ãƒ©ã‚¹ã‚¿æ•°: {clusterer.n_clusters}\")\n",
    "print(f\"\\nã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒ:\")\n",
    "print(df_sample['cluster'].value_counts().sort_index())\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆ\n",
    "stats = clusterer.get_cluster_stats()\n",
    "print(f\"\\nã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆæƒ…å ±:\")\n",
    "for cluster_id in sorted([k for k in stats.keys() if k != 'noise'])[:5]:\n",
    "    stat = stats[cluster_id]\n",
    "    print(f\"  Cluster {cluster_id}: {stat['size']} items, mean_distance={stat['mean_distance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97042c",
   "metadata": {},
   "source": [
    "## 7. æ¬¡å…ƒåœ§ç¸®ï¼ˆPCAï¼‰ã¨å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb95554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] PCA æ¬¡å…ƒåœ§ç¸®ä¸­...\")\n",
    "\n",
    "# PCA ã§2æ¬¡å…ƒã«åœ§ç¸®\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"[OK] PCA åœ§ç¸®å®Œäº†\")\n",
    "print(f\"èª¬æ˜åˆ†æ•£ï¼ˆPC1+PC2ï¼‰: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"PC1èª¬æ˜åˆ†æ•£: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"PC2èª¬æ˜åˆ†æ•£: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"2DåŸ‹ã‚è¾¼ã¿å½¢çŠ¶: {embeddings_2d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c97c28",
   "metadata": {},
   "source": [
    "## 8. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ•£å¸ƒå›³è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6406ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ\n",
    "plot_data = pd.DataFrame({\n",
    "    'x': embeddings_2d[:, 0],\n",
    "    'y': embeddings_2d[:, 1],\n",
    "    'text': df_sample['cleaned_text'].values,\n",
    "    'category': df_sample['category'].values,\n",
    "    'cluster': df_sample['cluster'].astype(str).values\n",
    "})\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹ã‚¿è‰²åˆ†ã‘ã®æ•£å¸ƒå›³\n",
    "fig = px.scatter(\n",
    "    plot_data,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    color='cluster',\n",
    "    hover_data={'text': ':<100', 'category': True},\n",
    "    title='Document Clustering Visualization (PCA + HDBSCAN)',\n",
    "    labels={'x': 'PC1', 'y': 'PC2'},\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    hover_name='category'\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white'))\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    hovermode='closest',\n",
    "    plot_bgcolor='#f8f9fa',\n",
    "    font=dict(family=\"Arial, sans-serif\", size=12)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"[OK] ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ•£å¸ƒå›³ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ\")\n",
    "print(f\"\\nğŸ’¡ Tips: ãƒ›ãƒãƒ¼ã™ã‚‹ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5c65d",
   "metadata": {},
   "source": [
    "## 9. ã‚«ãƒ†ã‚´ãƒªåˆ¥å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d88315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªè‰²åˆ†ã‘ã®æ•£å¸ƒå›³\n",
    "fig2 = px.scatter(\n",
    "    plot_data,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    color='category',\n",
    "    hover_data={'text': ':<100'},\n",
    "    title='Document Clustering by Category',\n",
    "    labels={'x': 'PC1', 'y': 'PC2'},\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    hover_name='category'\n",
    ")\n",
    "\n",
    "fig2.update_traces(\n",
    "    marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white'))\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    hovermode='closest',\n",
    "    plot_bgcolor='#f8f9fa'\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "print(f\"[OK] ã‚«ãƒ†ã‚´ãƒªåˆ¥å¯è¦–åŒ–ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db065944",
   "metadata": {},
   "source": [
    "## 10. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e508a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.cooccurrence import CooccurrenceNetwork\n",
    "\n",
    "print(\"[INFO] å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ä¸­...\")\n",
    "\n",
    "tokenized_docs = df_sample['tokens'].tolist()\n",
    "network = CooccurrenceNetwork(window_size=5, min_frequency=2)\n",
    "graph = network.build_network(tokenized_docs)\n",
    "\n",
    "print(f\"[OK] ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰å®Œäº†\")\n",
    "print(f\"ãƒãƒ¼ãƒ‰æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªï¼‰: {graph.number_of_nodes()}\")\n",
    "print(f\"ã‚¨ãƒƒã‚¸æ•°ï¼ˆå…±èµ·é–¢ä¿‚ï¼‰: {graph.number_of_edges()}\")\n",
    "\n",
    "# ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n",
    "top_keywords = network.get_keywords_by_frequency(top_n=15)\n",
    "print(f\"\\nå‡ºç¾é »åº¦ãƒˆãƒƒãƒ—15ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:\")\n",
    "print(top_keywords.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ecd1ce",
   "metadata": {},
   "source": [
    "## 11. å…±èµ·ã‚¨ãƒƒã‚¸åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc759d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒƒãƒ—å…±èµ·ã‚¨ãƒƒã‚¸\n",
    "top_edges = network.get_top_edges(top_n=15)\n",
    "print(\"å…±èµ·é »åº¦ãƒˆãƒƒãƒ—15ãƒšã‚¢:\")\n",
    "print(top_edges.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e34cdd",
   "metadata": {},
   "source": [
    "## 12. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbaf920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸­å¿ƒæ€§æŒ‡æ¨™ã‚’è¨ˆç®—\n",
    "degree = network.get_node_degree()\n",
    "strength = network.get_node_strength()\n",
    "betweenness = network.get_betweenness_centrality()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§æŒ‡æ¨™\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nã€æ¬¡æ•°ä¸­å¿ƒæ€§ (Degree Centrality)ã€‘\")\n",
    "print(\"ãƒãƒ¼ãƒ‰ãŒç›´çµã—ã¦ã„ã‚‹ä»–ã®ãƒãƒ¼ãƒ‰æ•°:\")\n",
    "top_degree = sorted(degree.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for word, deg in top_degree:\n",
    "    print(f\"  {word:20s} : {deg:3d}\")\n",
    "\n",
    "print(f\"\\nã€é‡ã¿ä»˜ãæ¬¡æ•° (Strength)ã€‘\")\n",
    "print(\"å…±èµ·é »åº¦ã‚’åŠ å‘³ã—ãŸæ¥ç¶šåº¦:\")\n",
    "top_strength = sorted(strength.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for word, str_val in top_strength:\n",
    "    print(f\"  {word:20s} : {str_val:8.1f}\")\n",
    "\n",
    "print(f\"\\nã€åª’ä»‹ä¸­å¿ƒæ€§ (Betweenness Centrality)ã€‘\")\n",
    "print(\"ãƒãƒ¼ãƒ‰é–“ã®æœ€çŸ­çµŒè·¯ä¸Šã«å‡ºç¾ã™ã‚‹é »åº¦:\")\n",
    "top_betweenness = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for word, bet_val in top_betweenness:\n",
    "    print(f\"  {word:20s} : {bet_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fed514",
   "metadata": {},
   "source": [
    "## 13. ã‚¯ãƒ©ã‚¹ã‚¿è¦ç´„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¯ãƒ©ã‚¹ã‚¿è¦ç´„\n",
    "summary = clusterer.get_cluster_summary(texts, top_n=3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for cluster_id in sorted([k for k in summary.keys()]):\n",
    "    if cluster_id == 'noise':\n",
    "        continue\n",
    "    info = summary[cluster_id]\n",
    "    print(f\"\\nã€Cluster {cluster_id}ã€‘\")\n",
    "    print(f\"  ã‚µã‚¤ã‚º: {info['size']} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n",
    "    print(f\"  ä»£è¡¨çš„ãªãƒ†ã‚­ã‚¹ãƒˆ:\")\n",
    "    for i, text in enumerate(info['representative_texts'][:2], 1):\n",
    "        preview = text[:70] + \"...\" if len(text) > 70 else text\n",
    "        print(f\"    {i}. {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a5b2f",
   "metadata": {},
   "source": [
    "## 14. çµ±è¨ˆã‚µãƒãƒªãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"åˆ†æçµæœã‚µãƒãƒªãƒ¼\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†:\")\n",
    "print(f\"  - å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(df)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n",
    "print(f\"  - åˆ†æå¯¾è±¡: {len(df_sample)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n",
    "print(f\"  - ã‚«ãƒ†ã‚´ãƒªæ•°: {df['category'].nunique()}\")\n",
    "\n",
    "print(f\"\\nğŸ”¤ ãƒ™ã‚¯ãƒˆãƒ«åŒ–:\")\n",
    "print(f\"  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: Sentence-Transformers\")\n",
    "print(f\"  - åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embeddings.shape[1]}\")\n",
    "print(f\"  - ãƒ¢ãƒ¼ãƒ‰: Mock (ãƒ†ã‚¹ãƒˆç”¨)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (HDBSCAN):\")\n",
    "print(f\"  - æ¤œå‡ºã‚¯ãƒ©ã‚¹ã‚¿æ•°: {clusterer.n_clusters}\")\n",
    "print(f\"  - ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆ: {(df_sample['cluster'] == -1).sum()}\")\n",
    "if clusterer.get_silhouette_score() is not None:\n",
    "    print(f\"  - ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°: {clusterer.get_silhouette_score():.3f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ æ¬¡å…ƒåœ§ç¸® (PCA):\")\n",
    "print(f\"  - åœ§ç¸®æ³•: PCA (UMAPã¯LLVMäº’æ›æ€§å•é¡Œã®ãŸã‚PCAã‚’ä½¿ç”¨)\")\n",
    "print(f\"  - æœ€çµ‚æ¬¡å…ƒ: 2D\")\n",
    "print(f\"  - èª¬æ˜åˆ†æ•£: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "print(f\"\\nğŸŒ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ:\")\n",
    "print(f\"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {graph.number_of_nodes()}\")\n",
    "print(f\"  - å…±èµ·é–¢ä¿‚ï¼ˆã‚¨ãƒƒã‚¸ï¼‰: {graph.number_of_edges()}\")\n",
    "print(f\"  - æœ€é »å‡ºå˜èª: {top_keywords.iloc[0]['word']} ({top_keywords.iloc[0]['frequency']} å›)\")\n",
    "\n",
    "print(f\"\\nâœ¨ å¯è¦–åŒ–:\")\n",
    "print(f\"  - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ•£å¸ƒå›³: Plotly\")\n",
    "print(f\"  - ãƒ›ãƒãƒ¼æ©Ÿèƒ½: ã‚ã‚Šï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰\")\n",
    "print(f\"  - ã‚«ãƒ†ã‚´ãƒªåˆ¥è¡¨ç¤º: ã‚ã‚Š\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… åˆ†æå®Œäº†ï¼\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
